import { OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf"
import { DocxLoader } from "@langchain/community/document_loaders/fs/docx";
import { TextLoader } from "@langchain/classic/document_loaders/fs/text"
import { CSVLoader } from "@langchain/community/document_loaders/fs/csv"
import { Document } from "@langchain/core/documents";
import * as dotenv from "dotenv";
import * as path from "path";

dotenv.config();

export interface IngestionOptions {
  filePath: string;
  chunkSize?: number;
  chunkOverlap?: number;
  metadata?: Record<string, any>;
}

export class IngestionService {
  private embeddings: OpenAIEmbeddings;
  private textSplitter: RecursiveCharacterTextSplitter;
  public vectorStore: MemoryVectorStore | null = null;

  constructor() {
    console.log("üîß [IngestionService] Initializing...");
    this.embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "text-embedding-3-small",
    });

    this.textSplitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000,
      chunkOverlap: 200,
    });
    console.log("‚úÖ [IngestionService] Initialized successfully");
  }

  private async loadDocument(filePath: string): Promise<Document[]> {
    console.log(`üìÑ [IngestionService] Loading document: ${filePath}`);
    const ext = path.extname(filePath).toLowerCase();
    let loader;

    switch (ext) {
      case ".pdf":
        loader = new PDFLoader(filePath);
        break;
      case ".docx":
        loader = new DocxLoader(filePath);
        break;
      case ".txt":
        loader = new TextLoader(filePath);
        break;
      case ".csv":
        loader = new CSVLoader(filePath);
        break;
      default:
        throw new Error(`Unsupported file type: ${ext}`);
    }

    const docs = await loader.load();
    console.log(`‚úÖ [IngestionService] Loaded ${docs.length} document(s), total content length: ${docs.reduce((acc, doc) => acc + doc.pageContent.length, 0)} chars`);
    return docs;
  }

  private async getVectorStore(): Promise<MemoryVectorStore> {
    if (!this.vectorStore) {
      console.log("üÜï [IngestionService] Creating new MemoryVectorStore");
      this.vectorStore = new MemoryVectorStore(this.embeddings);
      console.log("‚úÖ [IngestionService] MemoryVectorStore created successfully");
    } else {
      console.log("‚ôªÔ∏è [IngestionService] Reusing existing MemoryVectorStore");
    }
    return this.vectorStore;
  }

  async ingestDocument(options: IngestionOptions) {
    const { filePath, chunkSize, chunkOverlap, metadata } = options;
    console.log(`\nüöÄ [IngestionService] Starting ingestion for: ${filePath}`);

    if (chunkSize || chunkOverlap) {
      this.textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: chunkSize || 1000,
        chunkOverlap: chunkOverlap || 200,
      });
    }

    const docs = await this.loadDocument(filePath);

    if (metadata) {
      docs.forEach((doc) => {
        doc.metadata = { ...doc.metadata, ...metadata };
      });
    }

    console.log(`‚úÇÔ∏è [IngestionService] Splitting documents...`);
    const splitDocs = await this.textSplitter.splitDocuments(docs);
    console.log(`‚úÖ [IngestionService] Created ${splitDocs.length} chunks`);

    const vectorStore = await this.getVectorStore();
    console.log(`üíæ [IngestionService] Adding ${splitDocs.length} chunks to vector store...`);
    await vectorStore.addDocuments(splitDocs);
    
    // Verify the documents were added
    console.log(`‚úÖ [IngestionService] Documents added to vector store`);
    console.log(`üìä [IngestionService] Vector Store Status: ${this.vectorStore ? 'INITIALIZED' : 'NOT INITIALIZED'}`);

    return {
      success: true,
      chunksCount: splitDocs.length,
      message: `Successfully ingested ${splitDocs.length} chunks from ${path.basename(filePath)}`,
    };
  }

  async ingestMultipleDocuments(filePaths: string[], metadata?: Record<string, any>) {
    const results = [];
    let totalChunks = 0;

    for (const filePath of filePaths) {
      try {
        const result = await this.ingestDocument({ filePath, metadata });
        results.push({ file: path.basename(filePath), chunks: result.chunksCount });
        totalChunks += result.chunksCount;
      } catch (error: any) {
        console.error(`‚ùå [IngestionService] Error ingesting ${filePath}:`, error.message);
        results.push({ file: path.basename(filePath), chunks: 0, error: error.message });
      }
    }

    console.log(`\nüìä [IngestionService] Final Status:`);
    console.log(`   - Total chunks: ${totalChunks}`);
    console.log(`   - Vector Store: ${this.vectorStore ? '‚úÖ INITIALIZED' : '‚ùå NOT INITIALIZED'}`);

    return { success: true, totalChunks, results };
  }

  // Method to check vector store status
  getVectorStoreStatus() {
    const status = {
      initialized: this.vectorStore !== null,
      type: this.vectorStore?.constructor.name || 'null',
    };
    console.log(`üîç [IngestionService] Vector Store Status Check:`, status);
    return status;
  }
}

export const ingestionService = new IngestionService();